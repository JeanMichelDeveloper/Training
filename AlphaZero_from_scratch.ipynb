{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpGd+bxCVj9wd8um7tmIF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMichelDeveloper/Training/blob/main/AlphaZero_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tic-tic-toe game"
      ],
      "metadata": {
        "id": "z2OzFkBvi0Op"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fxy1l-jShVGA"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "from tqdm.notebook import trange\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define game and rules\n",
        "class TicTacToe:\n",
        "  #init game\n",
        "  def __init__(self):\n",
        "    self.row_count = 3\n",
        "    self.column_count = 3\n",
        "    self.action_size = self.row_count * self.column_count\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"TicTacToe\"\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    return np.zeros((self.row_count, self.column_count))\n",
        "\n",
        "  #update game with next move\n",
        "  def get_next_state(self, state, action, player):\n",
        "    row = action // self.column_count\n",
        "    column = action % self.column_count\n",
        "    state[row, column] = player\n",
        "    return state\n",
        "\n",
        "  #list valid moves\n",
        "  def get_valid_moves(self, state):\n",
        "    return (state.reshape(-1) == 0).astype(np.uint8)\n",
        "\n",
        "  #check if game is won\n",
        "  def check_win(self, state, action):\n",
        "    if action == None:\n",
        "      return False\n",
        "\n",
        "    row = action // self.column_count\n",
        "    column = action % self.column_count\n",
        "    player = state[row, column]\n",
        "\n",
        "    return (\n",
        "        np.sum(state[row, :]) == player * self.column_count\n",
        "        or np.sum(state[:, column]) == player * self.row_count\n",
        "        or np.sum(np.diag(state)) == player * self.row_count\n",
        "        or np.sum((np.diag(np.flip(state, axis = 0)))) == player * self.row_count)\n",
        "\n",
        "  #check if game is terminated\n",
        "  def get_value_and_terminated(self, state, action):\n",
        "    if self.check_win(state, action):\n",
        "      return 1, True\n",
        "    if np.sum(self.get_valid_moves(state)) == 0:\n",
        "      return 0, True\n",
        "    return 0, False\n",
        "\n",
        "  #change player\n",
        "  def get_opponent(self,player):\n",
        "    return -player\n",
        "  \n",
        "  #get opponent last play value (negative because it's the opponent play) \n",
        "  def get_opponent_value(self, value):\n",
        "    return -value\n",
        "  \n",
        "  #simulate move from player perspective\n",
        "  def change_perspective(self, state, player):\n",
        "    return state * player\n",
        "\n",
        "  #encode state for model\n",
        "  def get_encoded_state(self, state):\n",
        "    encoded_state = np.stack(\n",
        "        (state == -1, state == 0, state == 1)\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    if len(state.shape) == 3:\n",
        "      encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "\n",
        "    return encoded_state"
      ],
      "metadata": {
        "id": "gG1uP5zGjDsH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define game and rules\n",
        "class ConnectFour:\n",
        "  #init game\n",
        "  def __init__(self):\n",
        "    self.row_count = 6\n",
        "    self.column_count = 7\n",
        "    self.action_size = self.column_count\n",
        "    self.in_a_row = 4\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"ConnectFour\"\n",
        "\n",
        "  def get_initial_state(self):\n",
        "    return np.zeros((self.row_count, self.column_count))\n",
        "\n",
        "  #update game with next move\n",
        "  def get_next_state(self, state, action, player):\n",
        "    row = np.max(np.where(state[:, action] == 0))\n",
        "    state[row, action] = player\n",
        "    return state\n",
        "\n",
        "  #list valid moves\n",
        "  def get_valid_moves(self, state):\n",
        "    return (state[0] == 0).astype(np.uint8)\n",
        "\n",
        "  #check if game is won\n",
        "  def check_win(self, state, action):\n",
        "    if action == None:\n",
        "      return False\n",
        "\n",
        "    row = np.min(np.where(state[:, action] != 0))\n",
        "    column = action\n",
        "    player = state[row][column]\n",
        "\n",
        "    def count(offset_row, offset_column):\n",
        "      for i in range(1, self.in_a_row):\n",
        "        r = row + offset_row * i\n",
        "        c = action + offset_column * i\n",
        "        \n",
        "        if (\n",
        "            r < 0\n",
        "            or r >= self.row_count\n",
        "            or c < 0\n",
        "            or c >= self.column_count\n",
        "            or state[r][c] != player\n",
        "         ):\n",
        "\n",
        "          return i -1\n",
        "\n",
        "      return self.in_a_row - 1\n",
        "\n",
        "    return (\n",
        "        count(1, 0) >= self.in_a_row - 1 #vertical\n",
        "        or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 #horizontal\n",
        "        or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 #top left diagonal\n",
        "        or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
        "        )  \n",
        "\n",
        "\n",
        "  #check if game is terminated\n",
        "  def get_value_and_terminated(self, state, action):\n",
        "    if self.check_win(state, action):\n",
        "      return 1, True\n",
        "    if np.sum(self.get_valid_moves(state)) == 0:\n",
        "      return 0, True\n",
        "    return 0, False\n",
        "\n",
        "  #change player\n",
        "  def get_opponent(self,player):\n",
        "    return -player\n",
        "  \n",
        "  #get opponent last play value (negative because it's the opponent play) \n",
        "  def get_opponent_value(self, value):\n",
        "    return -value\n",
        "  \n",
        "  #simulate move from player perspective\n",
        "  def change_perspective(self, state, player):\n",
        "    return state * player\n",
        "\n",
        "  #encode state for model\n",
        "  def get_encoded_state(self, state):\n",
        "    encoded_state = np.stack(\n",
        "        (state == -1, state == 0, state == 1)\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    if len(state.shape) == 3:\n",
        "      encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
        "    \n",
        "    return encoded_state"
      ],
      "metadata": {
        "id": "cve9peNtRfEC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a tictactoe game\n",
        "tictactoe = TicTacToe()\n",
        "player = 1\n",
        "\n",
        "#initialise game\n",
        "state = tictactoe.get_initial_state()\n",
        "\n",
        "while True:\n",
        "  #get list of valid moves\n",
        "  print(state)\n",
        "  valid_moves = tictactoe.get_valid_moves(state)\n",
        "  print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
        "\n",
        "  #player choose next move\n",
        "  action = int(input(f\"{player}:\"))\n",
        "\n",
        "  #check if move is valid\n",
        "  if valid_moves[action] == 0:\n",
        "    print(\"action not valid\")\n",
        "    continue\n",
        "  \n",
        "  #update game\n",
        "  state = tictactoe.get_next_state(state, action, player)\n",
        "\n",
        "  value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
        "\n",
        "  #check if game is terminated\n",
        "  if is_terminal:\n",
        "    print(state)\n",
        "    if value == 1:\n",
        "      print(\"player\", player, \"won\")\n",
        "    else:\n",
        "      print(\"draw\")\n",
        "    break\n",
        "  \n",
        "  #change player\n",
        "  player = tictactoe.get_opponent(player)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "3CeviJVhmG5Z",
        "outputId": "11637aac-bb55-48a5-d4ad-82f413487b53"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9f086174b20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m#player choose next move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{player}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#check if move is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Monte Carlo Tree Search"
      ],
      "metadata": {
        "id": "NI09MTXdpVq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create model\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, game, num_resBlocks, num_hidden, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.startBlock = nn.Sequential(\n",
        "        nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(num_hidden),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.backBone = nn.ModuleList(\n",
        "        [ResBlock(num_hidden) for i in range(num_resBlocks)])\n",
        "    \n",
        "    self.policyHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
        "    )\n",
        "\n",
        "    self.valueHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3 * game.row_count * game.column_count, 1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.startBlock(x)\n",
        "\n",
        "    for resBlock in self.backBone:\n",
        "      x = resBlock(x)\n",
        "\n",
        "    policy = self.policyHead(x)\n",
        "    value = self.valueHead(x)\n",
        "\n",
        "    return policy, value\n",
        "\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, num_hidden):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "    self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.bn2(self.conv2(x))\n",
        "    x += residual\n",
        "    x = F.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xrX2Ant9btgc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "tictactoe = TicTacToe()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "state = tictactoe.get_next_state(state, 2, -1)\n",
        "state = tictactoe.get_next_state(state, 4, -1)\n",
        "state = tictactoe.get_next_state(state, 8, 1)\n",
        "state = tictactoe.get_next_state(state, 6, 1)\n",
        "\n",
        "encoded_state = tictactoe.get_encoded_state(state)\n",
        "\n",
        "tensor_state = torch.tensor(encoded_state, device = device).unsqueeze(0)\n",
        "\n",
        "model = ResNet(tictactoe, 4, 64, device = device)\n",
        "model.load_state_dict(torch.load('model_2.pt', map_location = device))\n",
        "model.eval()\n",
        "\n",
        "policy, value = model(tensor_state)\n",
        "\n",
        "value = value.item()\n",
        "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "print(value)\n",
        "print(state)\n",
        "print(tensor_state)\n",
        "\n",
        "plt.bar(range(tictactoe.action_size), policy)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "RDd_RMWrf3Cj",
        "outputId": "f7ef0cd9-780c-4bb2-928a-16ebf82ad9b3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-758d83b6bb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtictactoe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_2.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_2.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class to define nodes\n",
        "class Node:\n",
        "  def __init__(self, game, args, state, parent = None, action_taken = None, prior=0, visit_count = 0):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action_taken = action_taken\n",
        "    self.prior = prior\n",
        "\n",
        "    self.children = []\n",
        "    #self.expandable_moves = game.get_valid_moves(state)\n",
        "\n",
        "    self.visit_count = 0\n",
        "    self.value_sum = 0\n",
        "  \n",
        "  #check if node is fully expanded\n",
        "  def is_fully_expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  #select best child of node\n",
        "  def select(self):\n",
        "    best_child = None\n",
        "    best_ucb = -np.inf\n",
        "\n",
        "    for child in self.children:\n",
        "      ucb = self.get_ucb(child)\n",
        "      \n",
        "      if ucb > best_ucb:\n",
        "        best_child = child\n",
        "        best_ucb = ucb\n",
        "    \n",
        "    return best_child\n",
        "\n",
        "  #calculate ucb\n",
        "  def get_ucb(self, child):\n",
        "    if child.visit_count == 0:\n",
        "      q_value = 0\n",
        "    else:  \n",
        "      q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
        "    return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
        "\n",
        "  #expand node\n",
        "  def expand(self, policy):\n",
        "    for action, prob in enumerate(policy):\n",
        "      if prob > 0:\n",
        "\n",
        "        child_state = self.state.copy()\n",
        "        child_state = self.game.get_next_state(child_state, action, 1)\n",
        "        child_state = self.game.change_perspective(child_state, player = -1)\n",
        "\n",
        "        child = Node(self.game, self.args, child_state, self, action, prob)\n",
        "        self.children.append(child)\n",
        "\n",
        "    #return child\n",
        "\n",
        "  #simulate game\n",
        "  #def simulate(self):\n",
        "   # value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
        "    #value = self.game.get_opponent_value(value)\n",
        "\n",
        "    #if is_terminal:\n",
        "     # return value\n",
        "\n",
        "    #rollout_state = self.state.copy()\n",
        "    #rollout_player = 1\n",
        "\n",
        "    #while True:\n",
        "     # valid_moves = self.game.get_valid_moves(rollout_state)\n",
        "     # action = np.random.choice(np.where(valid_moves == 1)[0])\n",
        "     # rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
        "     # value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
        "\n",
        "     # if is_terminal:\n",
        "      #  if rollout_player == -1:\n",
        "       #   value = self.game.get_opponent_value(value)\n",
        "       # return value\n",
        "      \n",
        "     # rollout_player = self.game.get_opponent(rollout_player)\n",
        "\n",
        "  #backpropagate info up the tree\n",
        "  def backpropagate(self, value):\n",
        "    self.value_sum += value\n",
        "    self.visit_count += 1\n",
        "\n",
        "    value = self.game.get_opponent_value(value)\n",
        "\n",
        "    if self.parent is not None:\n",
        "      self.parent.backpropagate(value)"
      ],
      "metadata": {
        "id": "YgjWXGAopRZ5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class for MCTS\n",
        "class MCTS:\n",
        "  #initialize MCTS\n",
        "  def __init__(self, game, args, model):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.model = model\n",
        "\n",
        "  @torch.no_grad()\n",
        "\n",
        "  def search(self, state):\n",
        "    #define root\n",
        "    root = Node(self.game, self.args, state, visit_count = 1)\n",
        "\n",
        "    policy, _ = self.model(torch.tensor(self.game.get_encoded_state(state), device = self.model.device).unsqueeze(0))\n",
        "\n",
        "    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
        "\n",
        "    valid_moves = self.game.get_valid_moves(state)\n",
        "\n",
        "    policy *= valid_moves\n",
        "\n",
        "    policy /= np.sum(policy)\n",
        "\n",
        "    root.expand(policy)\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "      #selection\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminal:\n",
        "\n",
        "        policy, value = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(node.state), device = self.model.device).unsqueeze(0)\n",
        "        )\n",
        "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        valid_moves = self.game.get_valid_moves(node.state)\n",
        "\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "\n",
        "        value = value.item()\n",
        "\n",
        "        #expansion\n",
        "        node.expand(policy)\n",
        "\n",
        "        #simulation\n",
        "        #value = node.simulate()\n",
        "\n",
        "      #backpropagation\n",
        "      node.backpropagate(value)\n",
        "    \n",
        "    #return visit_counts\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "\n",
        "    action_probs /= np.sum(action_probs)\n",
        "\n",
        "    return action_probs"
      ],
      "metadata": {
        "id": "Z3kTJ6sWAC0B"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create AlphaZero"
      ],
      "metadata": {
        "id": "CfZHA099A0nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZero:\n",
        "  def __init__(self, model, optimizer, game, args):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.game = game\n",
        "    self.args =args\n",
        "    self.mcts = MCTS(game, args, model)\n",
        "  \n",
        "  #Make the model play against itself\n",
        "  def selfPlay(self):\n",
        "    memory = []\n",
        "    player = 1\n",
        "    state = self.game.get_initial_state()\n",
        "\n",
        "    while True:\n",
        "      neutral_state = self.game.change_perspective(state, player)\n",
        "      action_probs = self.mcts.search(neutral_state)\n",
        "\n",
        "      memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "      temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "\n",
        "      action = np.random.choice(self.game.action_size, p = action_probs)\n",
        "\n",
        "      state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "      value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "      if is_terminal:\n",
        "        returnMemory = []\n",
        "\n",
        "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "          hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "\n",
        "          returnMemory.append((\n",
        "              self.game.get_encoded_state(hist_neutral_state),\n",
        "              hist_action_probs,\n",
        "              hist_outcome\n",
        "          ))\n",
        "\n",
        "        return returnMemory\n",
        "\n",
        "    player = self.game.get_opponent(player)\n",
        "\n",
        "  #train model\n",
        "  def train(self, memory):\n",
        "    random.shuffle(memory)\n",
        "\n",
        "    for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "      sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "      state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "      state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "      state = torch.tensor(state, dtype = torch.float32, device = self.model.device)\n",
        "      policy_targets = torch.tensor(policy_targets, dtype = torch.float32, device = self.model.device)\n",
        "      value_targets = torch.tensor(value_targets, dtype = torch.float32, device = self.model.device)\n",
        "\n",
        "      out_policy, out_value = self.model(state)\n",
        "\n",
        "      policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "      value_loss = F.mse_loss(out_value, value_targets)\n",
        "      loss = policy_loss + value_loss\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  #make model learn continuously\n",
        "  def learn(self):\n",
        "    for iteration in range(self.args['num_iterations']):\n",
        "      memory = []\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
        "        memory += self.selfPlay()\n",
        "\n",
        "      self.model.train()\n",
        "\n",
        "      for epoch in trange(self.args['num_epochs']):\n",
        "        self.train(memory)\n",
        "\n",
        "      torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
      ],
      "metadata": {
        "id": "NagWXCRfAwvS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MCTS with parallelization\n",
        "class MCTSParallel:\n",
        "  #initialize MCTS\n",
        "  def __init__(self, game, args, model):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.model = model\n",
        "\n",
        "  @torch.no_grad()\n",
        "\n",
        "  def search(self, states, spGames):\n",
        "    #define root\n",
        "    policy, _ = self.model(torch.tensor(self.game.get_encoded_state(states), device = self.model.device))\n",
        "\n",
        "    policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "\n",
        "    policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size = policy.shape[0])\n",
        "\n",
        "    for i, spg in enumerate(spGames):\n",
        "      spg_policy = policy[i]\n",
        "\n",
        "      valid_moves = self.game.get_valid_moves(states[i])\n",
        "\n",
        "      spg_policy *= valid_moves\n",
        "\n",
        "      spg_policy /= np.sum(spg_policy)\n",
        "\n",
        "      spg.root = Node(self.game, self.args, states[i], visit_count = 1)\n",
        "\n",
        "      spg.root.expand(spg_policy)\n",
        "\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      for spg in spGames:\n",
        "        spg.node = None\n",
        "        node = spg.root\n",
        "        #selection\n",
        "        while node.is_fully_expanded():\n",
        "          node = node.select()\n",
        "\n",
        "        value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "\n",
        "        value = self.game.get_opponent_value(value)\n",
        "\n",
        "        if is_terminal:\n",
        "          node.backpropagate(value)\n",
        "\n",
        "        else:\n",
        "          spg.node = node\n",
        "\n",
        "      expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
        "\n",
        "      if len(expandable_spGames) > 0:\n",
        "        states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
        "\n",
        "        policy, value = self.model(\n",
        "                torch.tensor(self.game.get_encoded_state(states), device = self.model.device)\n",
        "            )\n",
        "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
        "      \n",
        "      for i, mappingIdx in enumerate(expandable_spGames):\n",
        "        node = spGames[mappingIdx].node\n",
        "        spg_policy, spg_value = policy[i], value[i]\n",
        "\n",
        "        valid_moves = self.game.get_valid_moves(node.state)\n",
        "\n",
        "        spg_policy *= valid_moves\n",
        "        spg_policy /= np.sum(spg_policy)\n",
        "\n",
        "        #expansion\n",
        "        node.expand(spg_policy)\n",
        "\n",
        "        #backpropagation\n",
        "        node.backpropagate(spg_value)"
      ],
      "metadata": {
        "id": "3YeIHcYqLaFy"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AlphaZero with parallelization\n",
        "class AlphaZeroParallel:\n",
        "  def __init__(self, model, optimizer, game, args):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.game = game\n",
        "    self.args =args\n",
        "    self.mcts = MCTSParallel(game, args, model)\n",
        "  \n",
        "  #Make the model play against itself\n",
        "  def selfPlay(self):\n",
        "    return_memory = []\n",
        "    player = 1\n",
        "    spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
        "\n",
        "    while len(spGames) > 0:\n",
        "      states = np.stack([spg.state for spg in spGames])\n",
        "\n",
        "      neutral_states = self.game.change_perspective(states, player)\n",
        "\n",
        "      self.mcts.search(neutral_states, spGames)\n",
        "\n",
        "      for i in range(len(spGames))[::-1]:\n",
        "        spg = spGames[i]\n",
        "\n",
        "        action_probs = np.zeros(self.game.action_size)\n",
        "\n",
        "        for child in spg.root.children:\n",
        "          action_probs[child.action_taken] = child.visit_count\n",
        "\n",
        "        action_probs /= np.sum(action_probs)\n",
        "\n",
        "\n",
        "        spg.memory.append((spg.root.state, action_probs, player))\n",
        "\n",
        "        temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
        "\n",
        "        action = np.random.choice(self.game.action_size, p = action_probs)\n",
        "\n",
        "        spg.state = self.game.get_next_state(spg.state, action, player)\n",
        "\n",
        "        value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
        "\n",
        "        if is_terminal:\n",
        "\n",
        "          for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
        "            hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "\n",
        "            return_memory.append((\n",
        "                self.game.get_encoded_state(hist_neutral_state),\n",
        "                hist_action_probs,\n",
        "                hist_outcome\n",
        "            ))\n",
        "          \n",
        "          del spGames[i]\n",
        "\n",
        "      player = self.game.get_opponent(player)\n",
        "\n",
        "    return return_memory\n",
        "\n",
        "  #train model\n",
        "  def train(self, memory):\n",
        "    random.shuffle(memory)\n",
        "\n",
        "    for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
        "      sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
        "      state, policy_targets, value_targets = zip(*sample)\n",
        "\n",
        "      state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
        "\n",
        "      state = torch.tensor(state, dtype = torch.float32, device = self.model.device)\n",
        "      policy_targets = torch.tensor(policy_targets, dtype = torch.float32, device = self.model.device)\n",
        "      value_targets = torch.tensor(value_targets, dtype = torch.float32, device = self.model.device)\n",
        "\n",
        "      out_policy, out_value = self.model(state)\n",
        "\n",
        "      policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
        "      value_loss = F.mse_loss(out_value, value_targets)\n",
        "      loss = policy_loss + value_loss\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "  #make model learn continuously\n",
        "  def learn(self):\n",
        "    for iteration in range(self.args['num_iterations']):\n",
        "      memory = []\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
        "        memory += self.selfPlay()\n",
        "\n",
        "      self.model.train()\n",
        "\n",
        "      for epoch in trange(self.args['num_epochs']):\n",
        "        self.train(memory)\n",
        "\n",
        "      torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
        "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
        "\n",
        "\n",
        "class SPG:\n",
        "  def __init__(self, game):\n",
        "    self.state = game.get_initial_state()\n",
        "    self.memory = []\n",
        "    self.root = None\n",
        "    self.node = None"
      ],
      "metadata": {
        "id": "xapgDFOMLIA6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define arguments for the AlphaZero instance\n",
        "#game = TicTacToe()\n",
        "game = ConnectFour()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet(game, 9, 128, device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
        "args = {\n",
        "     'C': 2,\n",
        "     'num_searches': 600,\n",
        "     'num_iterations': 8,\n",
        "     'num_selfPlay_iterations': 500,\n",
        "     'num_parallel_games': 100,\n",
        "     'num_epochs': 4,\n",
        "     'batch_size': 128,\n",
        "     'temperature': 1.25,\n",
        "     'dirichlet_epsilon': 0.25,\n",
        "     'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "#Define AlphaZero instance\n",
        "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
        "alphaZero.learn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mbE0xxPHGjOM",
        "outputId": "a4d2c62b-dd60-42f5-c4fe-b57b1e0b9556"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8cad0bea4bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#define arguments for the AlphaZero instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#game = TicTacToe()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectFour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ConnectFour' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TicTacToe game with MCTS"
      ],
      "metadata": {
        "id": "sdu8fnuma343"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a game with MCTS\n",
        "#game = TicTacToe()\n",
        "game = ConnectFour()\n",
        "player = 1\n",
        "\n",
        "#define MCTS arguments\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 100,\n",
        "    'dirichlet_epsilon': 0.0,\n",
        "    'dirichlet_alpha': 0.3\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#initialise model\n",
        "model = ResNet(game, 9, 128, device)\n",
        "model.load_state_dict(torch.load(\"model_0_ConnectFour.pt\", map_location = device))\n",
        "model.eval()\n",
        "\n",
        "#initialise MCTS player\n",
        "mcts = MCTS(game, args, model)\n",
        "\n",
        "#initialise game\n",
        "state = game.get_initial_state()\n",
        "\n",
        "while True:\n",
        "  print(state)\n",
        "\n",
        "  #player 1 move\n",
        "  if player == 1:\n",
        "    #get list of valid moves\n",
        "    valid_moves = game.get_valid_moves(state)\n",
        "    print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
        "\n",
        "    #player choose next move\n",
        "    action = int(input(f\"{player}:\"))\n",
        "\n",
        "    #check if move is valid\n",
        "    if valid_moves[action] == 0:\n",
        "      print(\"action not valid\")\n",
        "      continue\n",
        "  \n",
        "  #MCTS move\n",
        "  else:\n",
        "    neutral_state = game.change_perspective(state,player)\n",
        "    mcts_probs = mcts.search(neutral_state)\n",
        "    action = np.argmax(mcts_probs)\n",
        "\n",
        "  #update game\n",
        "  state = game.get_next_state(state, action, player)\n",
        "\n",
        "  value, is_terminal = game.get_value_and_terminated(state, action)\n",
        "\n",
        "  #check if game is terminated\n",
        "  if is_terminal:\n",
        "    print(state)\n",
        "    if value == 1:\n",
        "      print(\"player\", player, \"won\")\n",
        "    else:\n",
        "      print(\"draw\")\n",
        "    break\n",
        "  \n",
        "  #change player\n",
        "  player = game.get_opponent(player)"
      ],
      "metadata": {
        "id": "TS-B00FwAS-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}